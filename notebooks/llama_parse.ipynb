{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multimodal Retrieval Augmented Generation (RAG) with Llama Parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Dependencies, Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/joaocosentino/.pyen\n",
      "[nltk_data]     v/versions/user_manual/lib/python3.10/site-\n",
      "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'field_validator' from 'llama_index.core.bridge.pydantic' (/home/joaocosentino/.pyenv/versions/user_manual/lib/python3.10/site-packages/llama_index/core/bridge/pydantic.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_parse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaParse\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Settings,SimpleDirectoryReader,StorageContext,VectorStoreIndex\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnode_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MarkdownElementNodeParser\n",
      "File \u001b[0;32m~/.pyenv/versions/user_manual/lib/python3.10/site-packages/llama_parse/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_parse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LlamaParse, ResultType\n\u001b[1;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlamaParse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResultType\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/user_manual/lib/python3.10/site-packages/llama_parse/base.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfsspec\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspec\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AbstractBufferedFile\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01masync_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_jobs\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbridge\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, field_validator\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DEFAULT_BASE_URL\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BasePydanticReader\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'field_validator' from 'llama_index.core.bridge.pydantic' (/home/joaocosentino/.pyenv/versions/user_manual/lib/python3.10/site-packages/llama_index/core/bridge/pydantic.py)"
     ]
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import Settings,SimpleDirectoryReader,StorageContext,VectorStoreIndex\n",
    "from llama_index.core.node_parser import MarkdownElementNodeParser\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.vector_stores.kdbai import KDBAIVectorStore\n",
    "from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "from getpass import getpass\n",
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting Needed APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "load_dotenv('../.env')\n",
    "llama_cloud = os.getenv('LLMA_CLOUD_API')\n",
    "open_ai = os.getenv('OPENAI_API')\n",
    "groq = os.getenv('GROQ_API')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setting up Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_collection(file_path, collection_name):\n",
    "  '''\n",
    "\n",
    "  '''\n",
    "  load_dotenv('.env')\n",
    "\n",
    "  storage_path = os.getenv('STORAGE_PATH')\n",
    "  if storage_path is None:\n",
    "      raise ValueError('STORAGE_PATH environment variable is not set')\n",
    "\n",
    "  if not os.path.isdir(storage_path):\n",
    "    raise NotADirectoryError('STORAGE_PATH must be a repository')\n",
    "\n",
    "  # Local PDF file uploads\n",
    "  print(\"Reading pdf...\")\n",
    "  loader = UnstructuredPDFLoader(file_path=file_path)\n",
    "  data = loader.load()\n",
    "  print(\"Done!\")\n",
    "\n",
    "  # Split and chunk\n",
    "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=100)\n",
    "  chunks = text_splitter.split_documents(data)\n",
    "\n",
    "  # Add to vector database\n",
    "  vector_db = Chroma.from_documents(\n",
    "      documents=chunks,\n",
    "      embedding=embeddings,\n",
    "      collection_name=collection_name,\n",
    "      persist_directory=storage_path\n",
    "  )\n",
    "\n",
    "  print(f'File {file_path} uploaded to collection {collection_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LlamaParse & LlamaIndex Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 31823.25it/s]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_MODEL  = \"nomic-embed-text\"\n",
    "GENERATION_MODEL = \"mistral\"\n",
    "\n",
    "\n",
    "# LLM from Ollama\n",
    "local_model = \"mistral\"\n",
    "# llm_local = ChatOllama(model=local_model)\n",
    "\n",
    "llm_local = Groq(model=\"mixtral-8x7b-32768\", api_key= groq)\n",
    "# embed_model = OllamaEmbeddings(model=EMBEDDING_MODEL,show_progress=True)\n",
    "embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "Settings.llm = llm_local\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_pdf = \"../pdf_files/owner_manual_p283-p300.pdf\"\n",
    "pdf_path = '../pdf_files/owner_manual_full.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsing_instructions = '''You will answer questions about information that can be found in the owner's manual of the RAM 1500 vehicle, model year 2025, Crew Cab version.\n",
    "    Your task is to generate five different versions of the given user question to retrieve relevant documents\n",
    "    from a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = \"\"\"\n",
    "You are a highly proficient language model designed to convert pages from PDF, PPT and other files into structured markdown text. Your goal is to accurately transcribe text, represent formulas in LaTeX MathJax notation, and identify and describe images, particularly graphs and other graphical elements.\n",
    "\n",
    "You have been tasked with creating a markdown copy of each page from the provided PDF or PPT image. Each image description must include a full description of the content, a summary of the graphical object.\n",
    "\n",
    "Maintain the sequence of all the elements.\n",
    "\n",
    "For the following element, follow the requirement of extraction:\n",
    "for Text:\n",
    "   - Extract all readable text from the page.\n",
    "   - Exclude any diagonal text, headers, and footers.\n",
    "\n",
    "for Text which includes hyperlink:\n",
    "    -Extract hyperlink and present it with the text\n",
    "\n",
    "for Formulas:\n",
    "   - Identify and convert all formulas into LaTeX MathJax notation.\n",
    "\n",
    "for Image Identification and Description:\n",
    "   - Identify all images, graphs, and other graphical elements on the page.\n",
    "   - If image contains wording that is hard to extract , flag it with <unidentifiable section> instead of parsing.\n",
    "   - For each image, include a full description of the content in the alt text, followed by a brief summary of the graphical object.\n",
    "   - If the image has a subtitle or caption, include it in the description.\n",
    "   - If the image has a formula convert it into LaTeX MathJax notation.\n",
    "   - If the image has a organisation chart , convert it into a hierachical understandable format.\n",
    "   - for graph , extract the value in table form as markdown representation\n",
    "\n",
    "\n",
    "# OUTPUT INSTRUCTIONS\n",
    "\n",
    "- Ensure all formulas are in LaTeX MathJax notation.\n",
    "- Exclude any diagonal text, headers, and footers from the output.\n",
    "- For each image and graph, provide a detailed description and summary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Parse the document with LlamaParse into markdown format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 779544e4-0959-4339-b643-cc5530ba2337\n"
     ]
    }
   ],
   "source": [
    "documents = LlamaParse(api_key=llama_cloud,result_type=\"markdown\",parsing_instructions=ins).load_data(short_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Extract Text and Table nodes from Markdown Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 2998.07it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 10782.27it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 6553.60it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:00, 8756.38it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "node_parser = MarkdownElementNodeParser(llm=llm_local,num_workers=8).from_defaults()\n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "base_nodes, objects = node_parser.get_nodes_and_objects(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Use a Reranker to improve retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_index = VectorStoreIndex(nodes=base_nodes+objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.postprocessor.flag_embedding_reranker import FlagEmbeddingReranker\n",
    "\n",
    "reranker = FlagEmbeddingReranker(\n",
    "    top_n=5,\n",
    "    model=\"BAAI/bge-reranker-large\",\n",
    ")\n",
    "\n",
    "recursive_query_engine = recursive_index.as_query_engine(\n",
    "    similarity_top_k=15,\n",
    "    node_postprocessors=[reranker],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;2;11;159;203mRetrieval entering 32cc2375-fff9-4404-9241-be54f33ea61d: TextNode\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query can you tell me which LLM are you based on?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering e1078670-a011-49fe-88b0-921aa4959b8b: TextNode\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query can you tell me which LLM are you based on?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering 25f5be44-08fb-4f7a-9979-8776a4bcfa03: TextNode\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query can you tell me which LLM are you based on?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203mRetrieval entering ae5caff0-cb51-43e3-bb91-063d798bc25f: TextNode\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200mRetrieving from object TextNode with query can you tell me which LLM are you based on?\n",
      "\u001b[0mBased on the provided context, there is no information about an \"LLM\" or any system, organization, or entity by that acronym. Therefore, I cannot determine any association with an LLM.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "query = \"can you tell me which LLM are you based on?\"\n",
    "response = recursive_query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "dir = \"../pdf_files\"\n",
    "reader = SimpleDirectoryReader(input_dir=dir)\n",
    "docs = reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "user_manual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
